{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4756,
     "status": "ok",
     "timestamp": 1568295804372,
     "user": {
      "displayName": "L Rowla",
      "photoUrl": "",
      "userId": "15048649825657049484"
     },
     "user_tz": -60
    },
    "id": "HA_hCb_KsoM8",
    "outputId": "c56bf38a-10c3-4b4e-f97b-f2a2e2a64a00"
   },
   "outputs": [],
   "source": [
    "# I GET ERRORS OF MORRPH WITH ANY BUT THE SIMPLEST\n",
    "# i have started from https://realpython.com/natural-language-processing-spacy-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t1H2vq0vsxdC"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "#nlp now refers to the language model instace\n",
    "# it doesnt give me the morphology errors if I add the disable here\n",
    "#nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\"])\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "about_text= \"we went to the bak for some money\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 94
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 18746,
     "status": "ok",
     "timestamp": 1567762594662,
     "user": {
      "displayName": "L Rowla",
      "photoUrl": "",
      "userId": "15048649825657049484"
     },
     "user_tz": -60
    },
    "id": "iAqcQ-ChtZl6",
    "outputId": "ef6de9eb-4a84-437b-8697-4a86a5d3227a"
   },
   "outputs": [],
   "source": [
    "#This is how you can convert a text file into a processed Doc object.\n",
    "about_doc = nlp(about_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 94
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 18746,
     "status": "ok",
     "timestamp": 1567762594662,
     "user": {
      "displayName": "L Rowla",
      "photoUrl": "",
      "userId": "15048649825657049484"
     },
     "user_tz": -60
    },
    "id": "iAqcQ-ChtZl6",
    "outputId": "ef6de9eb-4a84-437b-8697-4a86a5d3227a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', 'went', 'to', 'the', 'bak', 'for', 'some', 'money']\n"
     ]
    }
   ],
   "source": [
    "# Extract tokens for the given doc\n",
    "print ([token.text for token in about_doc])\n",
    "# assume that:Variable names ending with the suffix _text are Unicode string objects.\n",
    "# Variable name ending with the suffix _doc are spaCy’s language model objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LQp_bJYZuHxs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "we went to the bak for some money\n",
      "then \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# In spaCy, the sents property is used to extract sentences. \n",
    "# Here’s how you would extract the total number of sentences \n",
    "# and the sentences for a given input text:\n",
    "sentences = list(about_doc.sents)\n",
    "print (len(sentences))\n",
    "for sentence in sentences:\n",
    "    print (sentence)\n",
    "    print (\"then \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1116,
     "status": "ok",
     "timestamp": 1567508324013,
     "user": {
      "displayName": "L Rowla",
      "photoUrl": "",
      "userId": "15048649825657049484"
     },
     "user_tz": -60
    },
    "id": "uPbPZqJMxTLV",
    "outputId": "848369c0-a79d-44bc-adc0-b0a635f65d26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gus, can you, ...\n",
      "never mind, I forgot what I was saying.\n",
      "So, do you think we should ...\n",
      "Gus, can you, ... never mind, I forgot what I was saying.\n",
      "So, do you think we should ...\n"
     ]
    }
   ],
   "source": [
    "#Here’s an example, where an ellipsis(...) is used as the delimiter. \n",
    "#These sentences are still obtained via the sents attribute, a\n",
    "def set_custom_boundaries(doc):\n",
    "     # Adds support to use `...` as the delimiter for sentence detection\n",
    "     for token in doc[:-1]:\n",
    "         if token.text == '...':\n",
    "             doc[token.i+1].is_sent_start = True\n",
    "     return doc\n",
    "\n",
    "ellipsis_text = ('Gus, can you, ... never mind, I forgot'\n",
    "                  ' what I was saying. So, do you think'\n",
    "                  ' we should ...')\n",
    "# Load a new model instance\n",
    "custom_nlp = spacy.load('en_core_web_sm')\n",
    "custom_nlp.add_pipe(set_custom_boundaries, before='parser')\n",
    "custom_ellipsis_doc = custom_nlp(ellipsis_text)\n",
    "custom_ellipsis_sentences = list(custom_ellipsis_doc.sents)\n",
    "for sentence in custom_ellipsis_sentences:\n",
    "    print(sentence)\n",
    "# Sentence Detection with no customization\n",
    "ellipsis_doc = nlp(ellipsis_text)\n",
    "ellipsis_sentences = list(ellipsis_doc.sents)\n",
    "for sentence in ellipsis_sentences:\n",
    " print(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qi99h4fxyCue"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we 0\n",
      "went 3\n",
      "to 8\n",
      "the 11\n",
      "bak 15\n",
      "for 19\n",
      "some 23\n",
      "money 28\n"
     ]
    }
   ],
   "source": [
    "#Tokenization is the next step after sentence detection. \n",
    "#In spaCy, you can print tokens by iterating on the Doc object:\n",
    "for token in about_doc:\n",
    "    print (token, token.idx) \n",
    "#Note how spaCy preserves the starting index of the tokens. \n",
    "#It’s useful for in-place word replacement. \n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9mX9T182yZIl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we 0 we  True False False xx True\n",
      "went 3 went  True False False xxxx False\n",
      "to 8 to  True False False xx True\n",
      "the 11 the  True False False xxx True\n",
      "bak 15 bak  True False False xxx False\n",
      "for 19 for  True False False xxx True\n",
      "some 23 some  True False False xxxx True\n",
      "money 28 money True False False xxxx False\n"
     ]
    }
   ],
   "source": [
    "#spaCy provides various attributes for the Token class:\n",
    "for token in about_doc:\n",
    "     print (token, token.idx, token.text_with_ws,\n",
    "            token.is_alpha, token.is_punct, token.is_space,\n",
    "            token.shape_, token.is_stop)\n",
    "#In this example, some of the commonly required attributes are accessed:\n",
    "\n",
    "#text_with_ws prints token text with trailing space (if present).\n",
    "#is_alpha detects if the token consists of alphabetic characters or not.\n",
    "#is_punct detects if the token is a punctuation symbol or not.\n",
    "#is_space detects if the token is a space or not.\n",
    "#shape_ prints out the shape of the word.\n",
    "#is_stop detects if the token is a stop word or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1306,
     "status": "ok",
     "timestamp": 1567763524852,
     "user": {
      "displayName": "L Rowla",
      "photoUrl": "",
      "userId": "15048649825657049484"
     },
     "user_tz": -60
    },
    "id": "CiaJ0llXy8wk",
    "outputId": "0910fe7d-10a1-453c-9a5f-aa2d4cff23a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('went', 1), ('bak', 1), ('money', 1)]\n",
      "['went', 'bak', 'money']\n"
     ]
    }
   ],
   "source": [
    "#I then jog on past You can also customize the tokenization process to detect tokens on custom characters\n",
    "# and stop words\n",
    "# tutorial is here btw https://realpython.com/natural-language-processing-spacy-python/\n",
    "#and lemmatization\n",
    "# in effect .....\n",
    "# Remove stop words and punctuation symbols\n",
    "words = [token.text for token in about_doc\n",
    "          if not token.is_stop and not token.is_punct]\n",
    "from collections import Counter\n",
    "word_freq = Counter(words)\n",
    "# 5 commonly occurring words with their frequencies\n",
    "common_words = word_freq.most_common(5)\n",
    "print (common_words)\n",
    "# Unique words\n",
    "unique_words = [word for (word, freq) in word_freq.items() if freq == 1]\n",
    "print (unique_words)\n",
    "# then i jump on pass POS tagging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lNFgXPAu0-vw"
   },
   "source": [
    "Rule-based matching is one of the steps in extracting information from unstructured text. It’s used to identify and extract tokens and phrases according to patterns (such as lowercase) and grammatical features (such as part of speech).\n",
    "\n",
    "Rule-based matching can use regular expressions to extract entities (such as phone numbers) from an unstructured text. It’s different from extracting text using regular expressions only in the sense that regular expressions don’t consider the lexical and grammatical attributes of the text.\n",
    "\n",
    "With rule-based matching, you can extract a first name and a last name, which are always proper nouns:\n",
    "There is also an example of extracting phone numbers which I avoided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1279,
     "status": "ok",
     "timestamp": 1567763536930,
     "user": {
      "displayName": "L Rowla",
      "photoUrl": "",
      "userId": "15048649825657049484"
     },
     "user_tz": -60
    },
    "id": "nwvrw5CU0r_q",
    "outputId": "5eaf03c8-2dd4-412b-bdd2-20dd4483603a"
   },
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "def extract_full_name(nlp_doc):\n",
    "    pattern = [{'POS': 'PROPN'}, {'POS': 'PROPN'}]\n",
    "    matcher.add('FULL_NAME', None, pattern)\n",
    "    matches = matcher(nlp_doc)\n",
    "    for match_id, start, end in matches:\n",
    "        span = nlp_doc[start:end]\n",
    "        return span.text\n",
    "\n",
    "extract_full_name(about_doc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fTszo6VZ1Jmt"
   },
   "source": [
    "ou can create a preprocessing function that takes text as input and applies the following operations:\n",
    "\n",
    "Lowercases the text\n",
    "Lemmatizes each token\n",
    "Removes punctuation symbols\n",
    "Removes stop words\n",
    "A preprocessing function converts text to an analyzable format. It’s necessary for most NLP tasks. Here’s an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4217,
     "status": "ok",
     "timestamp": 1567763570637,
     "user": {
      "displayName": "L Rowla",
      "photoUrl": "",
      "userId": "15048649825657049484"
     },
     "user_tz": -60
    },
    "id": "xCu0ntXP1QTx",
    "outputId": "efe31d8b-c733-4314-d612-fe71e94abb96"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['went', 'bak', 'money']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> def is_token_allowed(token):\n",
    "...     '''\n",
    "...         Only allow valid tokens which are not stop words\n",
    "...         and punctuation symbols.\n",
    "...     '''\n",
    "...     if (not token or not token.string.strip() or\n",
    "...         token.is_stop or token.is_punct):\n",
    "...         return False\n",
    "...     return True\n",
    "...\n",
    ">>> def preprocess_token(token):\n",
    "...     # Reduce token to its lowercase lemma form\n",
    "...     return token.lemma_.strip().lower()\n",
    "...\n",
    ">>> complete_filtered_tokens = [preprocess_token(token)\n",
    "...     for token in about_doc if is_token_allowed(token)]\n",
    ">>> complete_filtered_tokens\n",
    "#Note that the complete_filtered_tokens does not contain any stop word or punctuation symbols\n",
    "#symbols and consists of lemmatized lowercase tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YQNYxmbz19ih"
   },
   "source": [
    "There is then a section on dependency parsing , and navigating the consqeuent tree for it. I avoid the detail of this but then show a dependency print out further down......"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cVQEIFgw2SeY"
   },
   "source": [
    "Shallow Parsing\n",
    "Shallow parsing, or chunking, is the process of extracting phrases from unstructured text. Chunking groups adjacent tokens into phrases on the basis of their POS tags. There are some standard well-known chunks such as noun phrases, verb phrases, and prepositional phrases.\n",
    "\n",
    "Noun Phrase Detection\n",
    "A noun phrase is a phrase that has a noun as its head. It could also include other kinds of words, such as adjectives, ordinals, determiners. Noun phrases are useful for explaining the context of the sentence. They help you infer what is being talked about in the sentence.\n",
    "\n",
    "spaCy has the property noun_chunks on Doc object. You can use it to extract noun phrases:\n",
    "I avoided the next bit about verb phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4333,
     "status": "ok",
     "timestamp": 1567763699179,
     "user": {
      "displayName": "L Rowla",
      "photoUrl": "",
      "userId": "15048649825657049484"
     },
     "user_tz": -60
    },
    "id": "d0kB54vv2KrN",
    "outputId": "82207851-81dd-4bbd-eb7a-12da3cc607ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we\n",
      "the bak\n",
      "some money\n"
     ]
    }
   ],
   "source": [
    "\n",
    ">>> # Extract Noun Phrases\n",
    ">>> for chunk in about_doc.noun_chunks:\n",
    "...     print (chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GVVrm8LT2lYb"
   },
   "source": [
    "Named Entity Recognition (NER) is the process of locating named entities in unstructured text and then classifying them into pre-defined categories, such as person names, organizations, locations, monetary values, percentages, time expressions, and so on.\n",
    "\n",
    "You can use NER to know more about the meaning of your text. For example, you could use it to populate tags for a set of documents in order to improve the keyword search. You could also use it to categorize customer support tickets into relevant categories.\n",
    "\n",
    "spaCy has the property ents on Doc objects. You can use it to extract named entities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6680,
     "status": "ok",
     "timestamp": 1567763748032,
     "user": {
      "displayName": "L Rowla",
      "photoUrl": "",
      "userId": "15048649825657049484"
     },
     "user_tz": -60
    },
    "id": "6X6XB8YT2u4R",
    "outputId": "c57b7949-3080-4971-b5b1-9d22cfc82c58"
   },
   "outputs": [],
   "source": [
    "for ent in about_doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char,ent.label_, spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h7_dKwMu29Lp"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wh3J2one29RC"
   },
   "source": [
    "In the above example, ent is a Span object with various attributes:\n",
    "\n",
    "text gives the Unicode text representation of the entity.\n",
    "start_char denotes the character offset for the start of the entity.\n",
    "end_char denotes the character offset for the end of the entity.\n",
    "label_ gives the label of the entity.\n",
    "spacy.explain gives descriptive details about an entity label. The spaCy model has a pre-trained list of entity classes. \n",
    "\n",
    "\n",
    "spaCy comes with a built-in visualizer called displaCy. You can use it to visualize a dependency parse or named entities in a browser or a Jupyter notebook.You can use displaCy to visualize these entities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 566
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1935,
     "status": "ok",
     "timestamp": 1567763991786,
     "user": {
      "displayName": "L Rowla",
      "photoUrl": "",
      "userId": "15048649825657049484"
     },
     "user_tz": -60
    },
    "id": "IaTbBLM33ElK",
    "outputId": "e951a713-ec2a-4969-db8e-3699a439cf59"
   },
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-76411ef50c09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# it disconnects for large documents, so that is why I have added the indexes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdisplacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdisplacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabout_doc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m105000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m105020\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dep'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjupyter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mdisplacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabout_doc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m104000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m104020\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ent'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjupyter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/spacy/displacy/__init__.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(docs, style, page, minify, jupyter, options, manual)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mDoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSpan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mobj\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSpan\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mDoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSpan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE096\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/spacy/displacy/__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mDoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSpan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mobj\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSpan\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mDoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSpan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE096\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mspan.pyx\u001b[0m in \u001b[0;36mspacy.tokens.span.Span.as_doc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mspan.pyx\u001b[0m in \u001b[0;36mspacy.tokens.span.Span.vector.__get__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "#Note: Here’s how you can use displaCy in a Jupyter notebook:\n",
    "# first one gives dependencies\n",
    "# second one gives entitites!\n",
    "# it disconnects for large documents, so that is why I have added the indexes\n",
    "from spacy import displacy\n",
    "displacy.render(about_doc[105000:105020], style='dep', jupyter=True)\n",
    "displacy.render(about_doc[104000:104020], style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Spacy.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
