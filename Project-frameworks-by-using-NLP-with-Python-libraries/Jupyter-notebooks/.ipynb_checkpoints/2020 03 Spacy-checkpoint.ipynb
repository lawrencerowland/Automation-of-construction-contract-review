{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some from Spacy tutorial, some from https://realpython.com/natural-language-processing-spacy-python/\n",
    "file = open('Corpus_of_Paras.txt', 'r')\n",
    "text= file.read() \n",
    "text=text[0:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is how you can convert a text file into a processed Doc object.\n",
    "#nlp now refers to the language model instance\n",
    "# a convenient nomenclature would be using the suffix _text for Unicode string objects.\n",
    "# and the suffix _doc for spaCy’s language model objects\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\") #disable=[\"tagger\", \"parser\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OPTIONAL PREPROCESSING**\n",
    "spaCy allows you to customize tokenization by updating the tokenizer property on the nlp object:\n",
    "you can create a preprocessing function that takes text as input and applies the following operations:\n",
    "Lowercases the text\n",
    "Lemmatizes each token\n",
    "Removes punctuation symbols\n",
    "Removes stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    ">>> from spacy.tokenizer import Tokenizer\n",
    ">>> custom_nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    ">>> prefix_re = spacy.util.compile_prefix_regex(custom_nlp.Defaults.prefixes)\n",
    ">>> suffix_re = spacy.util.compile_suffix_regex(custom_nlp.Defaults.suffixes)\n",
    ">>> infix_re = re.compile(r'''[-~]''')\n",
    ">>> def customize_tokenizer(nlp):\n",
    "...     # Adds support to use `-` as the delimiter for tokenization\n",
    "...     return Tokenizer(nlp.vocab, prefix_search=prefix_re.search,\n",
    "...                      suffix_search=suffix_re.search,\n",
    "...                      infix_finditer=infix_re.finditer,\n",
    "...                      token_match=None\n",
    "...                      )\n",
    "\n",
    ">>> custom_nlp.tokenizer = customize_tokenizer(custom_nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> custom_tokenizer_doc = custom_nlp(text)\n",
    ">>> print([token.text for token in custom_tokenizer_doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1st STEPS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)\n",
    "print([t.text for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It has already been tokenised. \n",
    "# See instances of tokens\n",
    "print(doc[-1].text)   \n",
    "print(doc[0].text)         \n",
    "print(doc[1].text)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc[1:20].text)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See tokens for the given doc\n",
    "print ([token.text for token in doc])\n",
    "\n",
    "# Or similarly you can print tokens by iterating on the Doc object:\n",
    "\n",
    "#for token in doc:\n",
    "#    print (token, token.idx) \n",
    "\n",
    "#Note how spaCy preserves the starting index of the tokens. \n",
    "#It’s useful for in-place word replacement. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ALTERNATIVE OPTIONAL PREPROCESSING ** AT THIS POINT I DON'T UNDERSTAND HOW THE OUTPUT GETS SET AS PART OF THE MODEL. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> def is_token_allowed(token):\n",
    "...     '''\n",
    "...         Only allow valid tokens which are not stop words\n",
    "...         and punctuation symbols.\n",
    "...     '''\n",
    "...     if (not token or not token.string.strip() or\n",
    "...         token.is_stop or token.is_punct):\n",
    "...         return False\n",
    "...     return True\n",
    "...\n",
    ">>> def preprocess_token(token):\n",
    "...     # Reduce token to its lowercase lemma form\n",
    "...     return token.lemma_.strip().lower()\n",
    "...\n",
    ">>> complete_filtered_tokens = [preprocess_token(token)\n",
    "...     for token in doc if is_token_allowed(token)]\n",
    ">>> complete_filtered_tokens\n",
    "#Note that the complete_filtered_tokens does not contain any stop word or punctuation symbols\n",
    "#symbols and consists of lemmatized lowercase tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BASIC PREPROCESSING** only appears to be useful when doing some stats on word types etc. \n",
    "This seems to be done by creating a new list of words, rather than by changing the Spacy object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I then jog on past stop words and lemmatization\n",
    "# in effect .....\n",
    "# Remove stop words and punctuation symbols\n",
    "words = [token.text for token in doc\n",
    "          if not token.is_stop and not token.is_punct]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WORD FRQUENCIES ETC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('\\n', 81), ('\\n\\n', 72), ('security', 23), ('Nuclear', 17), ('Security', 13)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "word_freq = Counter(words)\n",
    "# 5 commonly occurring words with their frequencies\n",
    "common_words = word_freq.most_common(5)\n",
    "print (common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique words\n",
    "unique_words = [word for (word, freq) in word_freq.items() if freq == 1]\n",
    "print (unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOUN CHUNKS**\n",
    "spaCy has the property noun_chunks on Doc object. You can use it to extract noun phrases:Shallow parsing, or chunking, is the process of extracting phrases from unstructured text. Chunking groups adjacent tokens into phrases on the basis of their POS tags. There are some standard well-known chunks such as noun phrases, verb phrases, and prepositional phrases.\n",
    "\n",
    "Noun Phrase Detection: A noun phrase is a phrase that has a noun as its head. It could also include other kinds of words, such as adjectives, ordinals, determiners. Noun phrases are useful for explaining the context of the sentence. They help you infer what is being talked about in the sentence.I avoided the next bit about verb phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_chunks = list(doc.noun_chunks)\n",
    "print(noun_chunks[2].text)  \n",
    "print (noun_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FIND SENTENCES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(doc.sents)\n",
    "#assert len(sentences) == 3\n",
    "\n",
    "print (len(sentences)\n",
    "print(sentences[100].text) \n",
    "\n",
    "for sentence in sentences:\n",
    "    print ('-' + str(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPTIONAL ALTERNATIVE FOR SENTENCES\n",
    "#Here’s an example, where an ellipsis(...) is used as the delimiter. \n",
    "#These sentences are still obtained via the sents attribute, a\n",
    "def set_custom_boundaries(doc):\n",
    "     # Adds support to use `...` as the delimiter for sentence detection\n",
    "     for token in doc[:-1]:\n",
    "         if token.text == '...':\n",
    "             doc[token.i+1].is_sent_start = True\n",
    "     return doc\n",
    "\n",
    "# Load a new model instance\n",
    "custom_nlp = spacy.load('en_core_web_sm')\n",
    "custom_nlp.add_pipe(set_custom_boundaries, before='parser')\n",
    "custom_ellipsis_doc = custom_nlp(text)\n",
    "custom_ellipsis_sentences = list(custom_ellipsis_doc.sents)\n",
    "for sentence in custom_ellipsis_sentences:\n",
    "    print(\"--\",sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OPTIONAL REFERRING TO PARTICULAR PARTS OF THE TEXT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use hash values for any string, building up a vocabulary\n",
    "screening_hash = nlp.vocab.strings[\"screening\"]  \n",
    "screening_text = nlp.vocab.strings[screening_hash]  \n",
    "print(screening_hash, screening_text)\n",
    "print(doc[2].orth, screening_hash)  #presumably only comes back with the same numbers if it is the same word\n",
    "print(doc[2].text, screening_text)   #presumably only comes back with the same words if it is the same word\n",
    "\n",
    "necessary_hash = doc.vocab.strings.add(\"necessary\")  \n",
    "necessary_text = doc.vocab.strings[necessary_hash]  \n",
    "print(necessary_hash, necessary_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NAMED ENTITY RECOGNITION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONR GUIDE 64 73 ORG\n",
      "Unique Document ID 161 179 PERSON\n",
      "CNS 200 203 ORG\n",
      "March 2017 244 254 DATE\n",
      "March 2020 272 282 DATE\n",
      "David Pascoe 300 312 PERSON\n",
      "2 588 589 CARDINAL\n",
      "2 592 593 CARDINAL\n",
      "PURPOSE 596 603 ORG\n",
      "2 716 717 CARDINAL\n",
      "3 720 721 CARDINAL\n",
      "2 826 827 CARDINAL\n",
      "4 830 831 CARDINAL\n",
      "IAEA 850 854 ORG\n",
      "2 921 922 CARDINAL\n",
      "5 925 926 CARDINAL\n",
      "3 1023 1024 CARDINAL\n",
      "6 1027 1028 CARDINAL\n",
      "INSPECTORS 1041 1051 ORG\n",
      "3 1150 1151 CARDINAL\n",
      "SUPPLY 1158 1164 PERSON\n",
      "4 1244 1245 CARDINAL\n",
      "SUPPLY CHAIN MANAGEMENT SYSTEMS 1252 1283 ORG\n",
      "5 1354 1355 CARDINAL\n",
      "9 1358 1359 CARDINAL\n",
      "8 1490 1491 CARDINAL\n",
      "ABBREVIATIONS 1511 1524 ORG\n",
      "9 1608 1609 CARDINAL\n",
      "Office for Nuclear Regulation 1654 1683 ORG\n",
      "2017 1685 1689 DATE\n",
      "www.onr.org.uk 1735 1749 ORG\n",
      "Revision 3 1841 1851 FAC\n",
      "1 1859 1860 CARDINAL\n",
      "9 1864 1865 CARDINAL\n",
      "1 1906 1907 CARDINAL\n",
      "1.2 1949 1952 CARDINAL\n",
      "2.1 1955 1958 CARDINAL\n",
      "3.1 1961 1964 CARDINAL\n",
      "3.2 1967 1970 CARDINAL\n",
      "The Office for Nuclear Regulation 1973 2006 ORG\n",
      "ONR 2008 2011 ORG\n",
      "Security Assessment \n",
      " 2038 2059 ORG\n",
      "Reference 1 2079 2090 DATE\n",
      "Fundamental Security \n",
      "Principles 2116 2148 WORK_OF_ART\n",
      "ONR 2438 2441 ORG\n",
      "the Nuclear Industries \n",
      "Security Regulations (NISR 2448 2498 ORG\n",
      "2003 2500 2504 DATE\n",
      "2 2516 2517 CARDINAL\n",
      "NISR 2689 2693 ORG\n",
      "22 2706 2708 CARDINAL\n",
      "SyAPs 2738 2743 PRODUCT\n",
      "Cyber Security 2761 2775 WORK_OF_ART\n",
      "Information Assurance 2781 2802 ORG\n",
      "Sensitive Nuclear Information 2901 2930 ORG\n",
      "ONR 2994 2997 ORG\n",
      "2 3164 3165 CARDINAL\n",
      "PURPOSE 3169 3176 ORG\n",
      "SCOPE 3181 3186 ORG\n",
      "ONR 3440 3443 ORG\n",
      "ONR 3531 3534 ORG\n",
      "SyAPs 3705 3710 PRODUCT\n",
      "ONR 3797 3800 ORG\n",
      "3 3854 3855 CARDINAL\n",
      "NISR 4186 4190 ORG\n",
      "SNI 4234 4237 ORG\n",
      "NISR 4241 4245 ORG\n",
      "22 4559 4561 CARDINAL\n",
      "SNI 4595 4598 ORG\n",
      "ONR 4600 4603 ORG\n",
      "4 4757 4758 CARDINAL\n",
      "RELATIONSHIP 4762 4774 PERSON\n",
      "IAEA 4778 4782 ORG\n",
      "GUIDANCE 4801 4809 ORG\n",
      "4.1 4812 4815 CARDINAL\n",
      "the Physical Protection of Nuclear Material 4913 4956 ORG\n",
      "3 4976 4977 CARDINAL\n",
      "the IAEA Nuclear Security Fundamentals 4984 5022 ORG\n",
      "4 5034 5035 CARDINAL\n",
      "IAEA Technical Guidance 5076 5099 ORG\n",
      "Page 2 5175 5181 CARDINAL\n",
      "9 5185 5186 CARDINAL\n",
      "OFFICIAL  \n",
      "\n",
      " \n",
      "\n",
      " \n",
      " \n",
      "\n",
      "\f",
      "Office for Nuclear Regulation  \n",
      " \n",
      " \n",
      "\n",
      " 5189 5247 ORG\n",
      "5.1 5270 5273 CARDINAL\n",
      "5.2 5276 5279 CARDINAL\n",
      "4.2 5282 5285 CARDINAL\n",
      "Fundamental Principle J of the 5288 5318 WORK_OF_ART\n",
      "the Nuclear Security \n",
      "Fundamentals 5685 5719 WORK_OF_ART\n",
      "Essential Element 12 5741 5761 LAW\n",
      "3.12 5802 5806 CARDINAL\n",
      "Recommendations 6066 6081 GPE\n",
      "Nuclear Security Series 6112 6135 EVENT\n",
      "INFCIRC/225/Revision 6230 6250 ORG\n",
      "5 6272 6273 DATE\n",
      "ONR 6339 6342 ORG\n",
      "ONR 6510 6513 ORG\n",
      "SyDP 4.2 6631 6639 PRODUCT\n",
      "4 6683 6684 CARDINAL\n",
      "The HMG Security Policy Framework 6812 6845 ORG\n",
      "SPF 6847 6850 ORG\n",
      "6 6863 6864 CARDINAL\n",
      "Cabinet 6880 6887 ORG\n",
      "HMG 6921 6924 ORG\n",
      "third 6943 6948 ORDINAL\n",
      "HMG 6966 6969 ORG\n",
      "HMG 7041 7044 ORG\n",
      "SPF 7154 7157 ORG\n",
      "SyAPs 7192 7197 PRODUCT\n",
      "SNI 7321 7324 ORG\n",
      "5.3 7424 7427 CARDINAL\n",
      "The Classification Policy (Reference 7430 7466 ORG\n",
      "SNI 7500 7503 ORG\n",
      "6 7589 7590 CARDINAL\n",
      "ADVICE 7594 7600 ORG\n",
      "INSPECTORS 7604 7614 ORG\n",
      "6.1 7617 7620 CARDINAL\n",
      "Supply Chain Management 7665 7688 ORG\n",
      "SCM 7690 7693 ORG\n",
      "6.2 8086 8089 CARDINAL\n",
      "3 8345 8346 CARDINAL\n",
      "9 8350 8351 CARDINAL\n",
      "OFFICIAL  \n",
      "\n",
      " \n",
      "\n",
      " \n",
      " \n",
      "\n",
      "\f",
      "Office for Nuclear Regulation  \n",
      " \n",
      " \n",
      "\n",
      " 8354 8412 ORG\n",
      "6.4 8435 8438 CARDINAL\n",
      "7.1 8441 8444 CARDINAL\n",
      "7.2 8447 8450 CARDINAL\n",
      "7.3 8453 8456 CARDINAL\n",
      "4 8464 8465 CARDINAL\n",
      "SyDP 8525 8529 ORG\n",
      "4.2 8530 8533 PRODUCT\n",
      "CPNI 9318 9322 ORG\n",
      "Security Industry \n",
      "Association 9855 9885 ORG\n",
      "BS EN 9916 9921 ORG\n",
      "the Security Institute 9974 9996 ORG\n"
     ]
    }
   ],
   "source": [
    "#Named Entity Recognition (NER) is the process of locating named entities in unstructured text and then classifying them into pre-defined categories\n",
    "#such as person names, organizations, locations, monetary values, percentages, time expressions, and so on.\n",
    "#You can use NER to know more about the meaning of your text.\n",
    "#For example, you could use it to populate tags for a set of documents in order to improve the keyword search. \n",
    "#You could also use it to categorize customer support tickets into relevant categories.\n",
    "#spaCy has the property ents on Doc objects. You can use it to extract named entities:\n",
    "#recognise and update named entities i.e. add an entity\n",
    "from spacy.tokens import Span\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FB 0 2 ORG\n"
     ]
    }
   ],
   "source": [
    "#Adding specific entities\n",
    "doc1 = nlp(\"FB is hiring a new VP of global policy\")\n",
    "doc1.ents = [Span(doc, 0, 1, label=\"ORG\")]\n",
    "for ent in doc1.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ent = nlp(\"When Sebastian Thrun started working on self-driving cars at Google \"\n",
    "              \"in 2007, few people outside of the company took him seriously.\")\n",
    "displacy.render(doc_ent, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DEPENDENCY PARSING** OPTIONAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#there is then a section on dependency parsing , and navigating the consqeuent tree for it. \n",
    "#I avoid the detail of this but then show a dependency print out here\n",
    "#view a dependency parser\n",
    "# i seem to have to stop this for it to print out\n",
    "\n",
    "#syntactic dependencies\n",
    "#doc = nlp(\"When Sebastian Thrun started working on self-driving cars at Google \" \"in 2007, few people outside of the company took him seriously.\")\n",
    "dep_labels = []\n",
    "for token in doc:\n",
    "    while token.head != token:\n",
    "        dep_labels.append(token.dep_)\n",
    "        token = token.head\n",
    "print(dep_labels)\n",
    "\n",
    "doc_dep = nlp(\"This is a sentence.\")\n",
    "displacy.render(doc_dep, style=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SIMILARITY BETWEEN WORDS** OPTIONAL This would not be my go-to package for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the best results, this example should use the the en_vectors_web_lg model -as the small model doesnt have all the stuff\n",
    "doc = nlp(\"Apple and banana are similar. Pasta and hippo aren't.\")\n",
    "\n",
    "apple = doc[0]\n",
    "banana = doc[2]\n",
    "pasta = doc[6]\n",
    "hippo = doc[8]\n",
    "\n",
    "print(\"apple <-> banana\", apple.similarity(banana))\n",
    "print(\"pasta <-> hippo\", pasta.similarity(hippo))\n",
    "print(apple.has_vector, banana.has_vector, pasta.has_vector, hippo.has_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SAVING MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you’ve been modifying the pipeline, vocabulary, vectors and entities\n",
    "#or made updates to the model, you’ll eventually want to save your progress – for example, everything that’s in your nlp object.\n",
    "# simple and efficient serialisation\n",
    "from spacy.tokens import Doc\n",
    "from spacy.vocab import Vocab\n",
    "doc.to_disk(\"Example saving of spacy nlp object.bin\")\n",
    "new_doc = Doc(Vocab()).from_disk(\"Example saving of spacy nlp object.bin\")\n",
    "print (new_doc[1:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export to Numpy arrays #not checked\n",
    "from spacy.attrs import ORTH, LIKE_URL\n",
    "\n",
    "doc = nlp(\"Check out https://spacy.io\")\n",
    "for token in doc:\n",
    "    print(token.text, token.orth, token.like_url)\n",
    "\n",
    "attr_ids = [ORTH, LIKE_URL]\n",
    "doc_array = doc.to_array(attr_ids)\n",
    "print(doc_array.shape)\n",
    "print(len(doc), len(attr_ids))\n",
    "\n",
    "assert doc[0].orth == doc_array[0, 0]\n",
    "assert doc[1].orth == doc_array[1, 0]\n",
    "assert doc[0].like_url == doc_array[0, 1]\n",
    "\n",
    "assert list(doc_array[:, 1]) == [t.like_url for t in doc]\n",
    "print(list(doc_array[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STREAM PROCESSING *** This takes up too much memory at moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import list from CSV (could also do from txt)\n",
    "import os\n",
    "directory = '/Users/lawrence/'\n",
    "Corpus_CSV=[]\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.csv'):\n",
    "        with open(os.path.join(directory, filename)) as f:\n",
    "            content=f.read()\n",
    "            Corpus_CSV.append(content)\n",
    "            print ('\\n\\nFile',filename)\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Minibatched stream processing\n",
    "# .pipe streams input, and produces streaming output\n",
    "iter_texts = (Corpus_CSV[i % 3] for i in range(5))\n",
    "for i, doc in enumerate(nlp.pipe(iter_texts, batch_size=50)):\n",
    "    print (i,doc) # i added this in to show what can be done\n",
    "    assert doc.is_parsed\n",
    "    if i == 7:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SEEING WHAT IT KNOWS ABOUT TOKENS** optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spaCy provides various attributes for the Token class:\n",
    "for token in doc:\n",
    "     print (token, token.idx, token.text_with_ws, token.is_space, token.is_stop)\n",
    "#In this example, some of the commonly required attributes are accessed:\n",
    "#text_with_ws prints token text with trailing space (if present).\n",
    "#is_space detects if the token is a space or not.\n",
    "#is_stop detects if the token is a stop word or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get POS tags and flags\n",
    "Particular_token=doc[100]\n",
    "print (Particular_token)\n",
    "print(\"Fine-grained POS tag\", Particular_token.pos_, Particular_token.pos)\n",
    "print(\"Coarse-grained POS tag\", Particular_token.tag_, Particular_token.tag)\n",
    "print(\"Word shape\", Particular_token.shape_, Particular_token.shape)\n",
    "print(\"Alphabetic characters?\", Particular_token.is_alpha)\n",
    "print(\"Punctuation mark?\", Particular_token.is_punct)\n",
    "print(\"Digit?\", Particular_token.is_digit)\n",
    "print(\"Like a number?\", Particular_token.like_num)\n",
    "print(\"Like an email address?\", Particular_token.like_email)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**extract entities (such as phone numbers) from an unstructured text**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rule-based matching is one of the steps in extracting information from unstructured text. It’s used to identify and extract tokens and phrases according to patterns (such as lowercase) and grammatical features (such as part of speech).\n",
    "\n",
    "Rule-based matching can use regular expressions to extract entities (such as phone numbers) from an unstructured text. It’s different from extracting text using regular expressions only in the sense that regular expressions don’t consider the lexical and grammatical attributes of the text.\n",
    "\n",
    "With rule-based matching, you can extract a first name and a last name, which are always proper nouns:\n",
    "There is also an example of extracting phone numbers which I avoided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ONR GUIDE'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "def extract_full_name(nlp_doc):\n",
    "    pattern = [{'POS': 'PROPN'}, {'POS': 'PROPN'}]\n",
    "    matcher.add('FULL_NAME', None, pattern)\n",
    "    matches = matcher(nlp_doc)\n",
    "    for match_id, start, end in matches:\n",
    "        span = nlp_doc[start:end]\n",
    "        return span.text\n",
    "extract_full_name(doc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
