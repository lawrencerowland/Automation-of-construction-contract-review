{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<img src=\"xx.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1st Notebook example: Keyword and Topic modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Purpose](#Purpose)** | **[Motivation](#Motivation)** |**[Features](#Features)** |**[Prerequisites](#Prerequisites)** |**[Quick start](#Quick-start)** |**[How-to-use](#How-to-use)**|\n",
    "\n",
    "# Purpose \n",
    "\n",
    "**For any set of portfolio documents, automatically generate keywords and summaries per document, and generate a set of the main topics found. Accordingly, generate a data-model for the portfolio** \n",
    "\n",
    "\n",
    "# Motivation\n",
    "\n",
    "Projects have different business and function aspects. These notebooks can be used to run simple code on a set of pdf and text files that the user has collected for their particular business domain. \n",
    "\n",
    "Below is basic code that can be applied directly to  any portfolio documents. Here this code is worked through with an example of understanding the implications of the regulatory environment on project delivery in the nuclear sector.This can be replaced as the user generates her own results. \n",
    "\n",
    "For simplicity, this one starts from documents as text files and internet pages. Additional features can be sought in the other notebooks once first results have been generated for your portfolio documents. \n",
    "\n",
    "# Features\n",
    "- Summarises each document\n",
    "- finds keywords for each document\n",
    "- creates a similarity-search: for any new paragraph, it finds the most similar documents from the library\n",
    "- (propose a data-model for the portfolio, based upon these findings) ***(Status: incomplete)***\n",
    "\n",
    "# Prerequisites \n",
    "\n",
    "The following Python libraries are needed: Gensim, BS4. Also NLTK and SpaCy. Gensim requires NumPy and SciPy. NLTK requires Pandas.\n",
    "\n",
    "For example, if one chooses to do it in Conda:\n",
    "\n",
    "1. Install Miniconda\n",
    "\n",
    "1. Create GENSIM_ENV environment with an appropriate Python version\n",
    "\n",
    "1. Install Gensim dependencies. Currently these are NumPy then SciPy and then Gensim \n",
    "\n",
    "1. conda activate GENSIM_ENV . Then jupyter lab\n",
    "\n",
    "(one may need to also activate the Env again once in Jupyter lab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick-start\n",
    "\n",
    "It is suggested to move straight down to **[How-to-use](#How-to-use)** to get started.\n",
    "\n",
    "(Otherwise, Attributes and adjustments are included immediately below for ease of reference, but best consulted after using the notebook).\n",
    "\n",
    "The user can start from the markdown file (xx) showing and explaining the code, or can work through the notebook directly (xx). Some minor detail is hidden in the markdown file, but shown in the notebook.\n",
    "\n",
    "# Attributes\n",
    "\n",
    "- Uses GENSIM library\n",
    "- Starts from Text file rather than PDF\n",
    "- also starts from Internet page\n",
    "- creates corpus at Document level rather than paragraph level\n",
    "- for Topic model, starts from Bag of Words model, as an interim step only\n",
    "- It uses TFIDF model only as an interim step to LSI only\n",
    "- for similarity-search, it uses cosine similarity, based upon LSI model\n",
    "\n",
    "# Attributes you can find in the other notebooks\n",
    "- Using other libraries than Gensim\n",
    "- Starting from Text file\n",
    "- creates corpus at paragraph level\n",
    "- uses Bag of Words model to generate results directly for similarity search\n",
    "\n",
    "# Easy adjustments that can be made, but have not been shown\n",
    "- for Topic model, uses only Bag of Words model \n",
    "- for similarity-search, using cosine similarity, based upon TFIDF model\n",
    "- for topic model, uses methods other than LSI. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How-to-use\n",
    "\n",
    "## Installation\n",
    "\n",
    "First Check installation has been made, as per the [READme](https://github.com/lawrencerowland/Data-Model-for-Project-Frameworks/blob/master/Project-frameworks-by-using-NLP-with-Python-libraries/README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Gensim\n",
    "The code is provided for the specific modules to be imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import gensim\n",
    "import igraph\n",
    "from igraph import *\n",
    "from gensim.summarization import summarize\n",
    "from gensim.summarization import keywords\n",
    "from gensim.parsing.preprocessing import strip_multiple_whitespaces\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim.parsing.porter import PorterStemmer \n",
    "from gensim.summarization.keywords import get_graph\n",
    "# Note that Smart_open.gcs may not import successfully depending on Gensim set up. This doesnt matter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Directory to find the portfolio text files\n",
    "\n",
    "This code uses the OS module to select the file with the user's text-files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.DS_Store', 'Commissioning of security systems and infrastructure cns-tast-gd-4.4 cns-tast-gd-4.4.pdf.txt', 'Construction Assurance ns-tast-gd-076.pdf.txt', 'Control of processes involving nuclear matter ns-tast-gd-023.pdf.txt', 'Decommissioning ns-tast-gd-026.txt', 'Design Safety Assurance ns-tast-gd-057.pdf.txt', 'Fundamental Principles ns-tast-gd-004.pdf.txt', 'Guidance on the Demonstration of ALARP ns-tast-gd-005.pdf.txt', 'Management of Radioactive material ns-tast-gd-024.pdf.txt', 'nuclear construction sites cns-tast-gd-6.6.pdf.txt', 'Organisational Change ns-tast-gd-048.pdf.txt', 'oversight of items or services cns-tast-gd-4.3.pdf.txt', 'Probabilistic Safety Analysis.pdf.txt', 'Procedure Design and Administrative Controls.pdf.txt', 'Procurement cns-tast-gd-4.1 cns-tast-gd-4.1.pdf.txt', 'Reliability and resilience of the security system cns-tast-gd-5.1 cns-tast-gd-5.1.pdf.txt', 'Supplier capability cns-tast-gd-4.2.pdf.txt']\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "import os\n",
    "directory = '/Users/lawrence/Documents/GitHub/Data-Model-for-Project-Frameworks/Project-frameworks-by-using-NLP-with-Python-libraries/Text-files-as-generated-by-PDF-Miner'\n",
    "print (os.listdir(directory))\n",
    "# Change directory location for your particular set-up. Or if you want to just re-run this nuclear example, then you just need to change the reference to the high-level directories\n",
    "# If a mac, can copy the directory location by opening the Inspector for the folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect any Internet pages \n",
    "\n",
    "This is a (hidden) option. \n",
    "\n",
    "In order to compare the results with the Orange results, I have not run this block of code. \n",
    "\n",
    "In addition to the text files you have collected, there may be one or two internet pages which you want to add to the analysis. \n",
    "I have added this partly to show what happens when a document is added which is about the same subject, but comes at the subject from a very different angle. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import requests\n",
    "text = requests.get('https://archive.org/stream/ProjectManagementForTheOilAndGasIndustry/ProjectManagementForTheOilAndGasIndustry_djvu.txt').text\n",
    "text=text[70000:300000] #here I have stripped the front text by inspection only.\n",
    "#text=strip_multiple_whitespaces(text)  #may wish to turn this off for readability\n",
    "filename=\"PM_guidance_for_Energy_Projects\"\n",
    "f= open(filename+\".txt\",\"w+\") \n",
    "f.write(filename+\"\\n\"+text)\n",
    "f.close()\n",
    "#If you wish to collect many Internet pages, you will need to construct a loop.\n",
    "# You also will need to check how well the text is extracted from  particular sources\n",
    "# and consult the Python Requests module documentation if it needs adjustment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding keywords from each document\n",
    "As seen in the outputs, for example, for the first document, keywords around security and commissioning are proposed. \n",
    "Adjust the ratio of keywords to total content size if required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "File Commissioning of security systems and infrastructure cns-tast-gd-4.4 cns-tast-gd-4.4.pdf.txt\n",
      "Keywords: security\n",
      "securely\n",
      "secure\n",
      "commissioning\n",
      "commissioned\n",
      "\n",
      "File Construction Assurance ns-tast-gd-076.pdf.txt\n",
      "Keywords: design\n",
      "designer\n",
      "designers\n",
      "designed\n",
      "designs\n",
      "designing\n",
      "construction\n",
      "constructed\n",
      "construct\n",
      "safety\n",
      "contractor\n",
      "contractors\n",
      "materials\n",
      "material\n",
      "\n",
      "File Control of processes involving nuclear matter ns-tast-gd-023.pdf.txt\n",
      "Keywords: control\n",
      "controlled\n",
      "controls\n",
      "controlling\n",
      "safety\n",
      "nuclear\n",
      "\n",
      "File Decommissioning ns-tast-gd-026.txt\n",
      "Keywords: decommissioning\n",
      "decommissioned\n",
      "decommission\n",
      "safety\n",
      "regulation\n",
      "regulators\n",
      "regulating\n",
      "regulations\n",
      "regulator\n",
      "regulate\n",
      "operated\n",
      "operations\n",
      "operational\n",
      "operating\n",
      "operation\n",
      "operators\n",
      "operator\n",
      "operates\n",
      "appropriate\n",
      "appropriately\n",
      "appropriateness\n",
      "including\n",
      "include\n",
      "includes\n",
      "included\n",
      "\n",
      "File Design Safety Assurance ns-tast-gd-057.pdf.txt\n",
      "Keywords: design\n",
      "designs\n",
      "designed\n",
      "designers\n",
      "designated\n",
      "designer\n",
      "safety\n",
      "processes\n",
      "process\n",
      "requires\n",
      "requirements\n",
      "required\n",
      "requirement\n",
      "requiring\n",
      "require\n",
      "\n",
      "File Fundamental Principles ns-tast-gd-004.pdf.txt\n",
      "Keywords: safety\n",
      "saps\n",
      "sap\n",
      "principles\n",
      "principle\n",
      "\n",
      "File Guidance on the Demonstration of ALARP ns-tast-gd-005.pdf.txt\n",
      "Keywords: risks\n",
      "risk\n",
      "alarp\n",
      "safety\n",
      "standards\n",
      "standard\n",
      "onr\n",
      "practicable\n",
      "practice\n",
      "practicability\n",
      "practical\n",
      "practices\n",
      "\n",
      "File Management of Radioactive material ns-tast-gd-024.pdf.txt\n",
      "Keywords: waste\n",
      "wastes\n",
      "regulation\n",
      "regulators\n",
      "regulations\n",
      "regulate\n",
      "regulator\n",
      "regulates\n",
      "regulating\n",
      "including\n",
      "includes\n",
      "include\n",
      "management\n",
      "managed\n",
      "manages\n",
      "managing\n",
      "managers\n",
      "manage\n",
      "safety\n",
      "\n",
      "File nuclear construction sites cns-tast-gd-6.6.pdf.txt\n",
      "Keywords: security\n",
      "securely\n",
      "secure\n",
      "secured\n",
      "securing\n",
      "sites\n",
      "site\n",
      "nuclear\n",
      "\n",
      "File Organisational Change ns-tast-gd-048.pdf.txt\n",
      "Keywords: change\n",
      "changes\n",
      "changed\n",
      "onr\n",
      "safety\n",
      "arrangements\n",
      "arrange\n",
      "\n",
      "File oversight of items or services cns-tast-gd-4.3.pdf.txt\n",
      "Keywords: security\n",
      "securely\n",
      "nuclear\n",
      "\n",
      "File Probabilistic Safety Analysis.pdf.txt\n",
      "Keywords: psas\n",
      "psa\n",
      "include\n",
      "including\n",
      "included\n",
      "includes\n",
      "events\n",
      "event\n",
      "analysis\n",
      "modelling\n",
      "model\n",
      "models\n",
      "modelled\n",
      "operated\n",
      "operation\n",
      "operating\n",
      "operational\n",
      "operators\n",
      "operator\n",
      "operate\n",
      "operability\n",
      "operations\n",
      "specific\n",
      "specifically\n",
      "specification\n",
      "specifications\n",
      "\n",
      "File Procedure Design and Administrative Controls.pdf.txt\n",
      "Keywords: procedure\n",
      "procedures\n",
      "procedural\n",
      "safety\n",
      "operated\n",
      "operating\n",
      "operations\n",
      "operator\n",
      "operational\n",
      "operation\n",
      "operators\n",
      "operate\n",
      "\n",
      "File Procurement cns-tast-gd-4.1 cns-tast-gd-4.1.pdf.txt\n",
      "Keywords: security\n",
      "securely\n",
      "nuclear\n",
      "onr\n",
      "\n",
      "File Reliability and resilience of the security system cns-tast-gd-5.1 cns-tast-gd-5.1.pdf.txt\n",
      "Keywords: security\n",
      "securely\n",
      "secure\n",
      "nuclear\n",
      "\n",
      "File Supplier capability cns-tast-gd-4.2.pdf.txt\n",
      "Keywords: security\n",
      "securely\n",
      "nuclear\n"
     ]
    }
   ],
   "source": [
    "Keywords_for_corpus =[]\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.txt'):\n",
    "        with open(os.path.join(directory, filename)) as f:\n",
    "            \n",
    "            content = f.read()\n",
    "            \n",
    "            key_words=keywords(content, ratio=0.007)\n",
    "            \n",
    "            print ('\\nFile',filename)\n",
    "            print ('Keywords:',key_words)\n",
    "            \n",
    "            Keywords_for_corpus.append(key_words)\n",
    "    \n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic summarisation of the content of each document\n",
    "As seen in the outputs, for example, for the first document, keywords around security and commissioning are proposed. \n",
    "For example, with a little tidying, it is suggested that the first document (Commissioning of security systems and infrastructure) has the following key points:\n",
    "\n",
    "- The scale of the commissioning activity should be proportionate to the level of complexity and degree of impact that the project will have on the site or facility’s ability to meet its security outcome as defined in Annexes C and D of SyAPs. \n",
    "\n",
    "- This TAG is aimed at providing guidance to the inspector when assessing the adequacy of the commissioning arrangements demonstrated through the performance of the equipment (and those personnel who will manage, operate and maintain it) and the adequacy of the procedures to support the operation of that equipment.\n",
    "\n",
    "Adjust the ratio of summary length to total content size if required. \n",
    "Similarly, a summary of the whole corpus can also be prepared. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Corpus_of_Summaries =[]\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.txt'):\n",
    "        with open(os.path.join(directory, filename)) as f:\n",
    "            \n",
    "            content = f.read()\n",
    "            \n",
    "            summary=(filename+'Summary\\n'+summarize(content, ratio=0.01)) # or use e.g.word_count=20\n",
    "            \n",
    "            print ('File',filename)\n",
    "            print ('\\nSummary:',summary,\"\\n\")\n",
    "            \n",
    "            Corpus_of_Summaries.append(summary)\n",
    "            \n",
    "            #print (content[0:100]) # testing the content is coming through\n",
    "            # print(repr(summary)) #alternate version showing line breaks etc\n",
    "    \n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# To print a particular summary, enter a number from 0 to 15.  \n",
    "print(Corpus_of_Summaries[0]) # Could equally do this w full content, but starting simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#hide\n",
    "## Summarisation across the entire corpus\n",
    "\n",
    "This is a summary of the whole document. \n",
    "After cleaning it up and improving punctuation we get:\n",
    "\n",
    "- 'www.onr.org.uk/operational/tech_asst_guides on Nuclear Licensed Sites\n",
    "- Training and assuring personnel competence Licensee Core Safety and Intelligent Customer Capabilities Periodic Safety Reviews (PSR) \n",
    "- The Purpose, Scope and Content of Nuclear Safety Cases Reasonably Practicable)\n",
    "- Examination, Inspection, Maintenance and Testing of Items Important to Safety\n",
    "-  Guidance on the Demonstration of ALARP \n",
    "- Management of Radioactive Materials and Radioactive Waste \n",
    "- Duty Holder Management of Records \n",
    "- Radiological Protection \n",
    "- Organisational Change \n",
    "- Human Factors Integration \n",
    "- Human Machine Interface \n",
    "- Human Reliability Analysis \n",
    "- Challenge Culture, Independent Challenge Capability \n",
    "- Categorisation of Safety Functions and Classification of NS-TAST-GD-098 an Internal Regulation function) \n",
    "- Provision of Nuclear Safety Advice Land Quality Management Function and\n",
    "- Content of the Nuclear Baseline Licensee Design authority Capability Structures\n",
    "- Components Asset Management Staffing Levels and Task Organisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "Corpus_as_one_string=\" \"\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.txt'):\n",
    "        with open(os.path.join(directory, filename)) as f:\n",
    "            content = f.read()\n",
    "            Corpus_as_one_string+=content\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "Summary=summarize(Corpus_as_one_string, word_count=1000) #ratio=0.01)\n",
    "print (Summary)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results to a single document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking stock\n",
    "So far we have\n",
    "'Corpus_as_one_string',\n",
    "'Corpus_of_Summaries',\n",
    "'Keywords_for_corpus',\n",
    "(this can be seen from dir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-55-7a54b622e7ed>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-55-7a54b622e7ed>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    write(Corpus_as_one_string+\"\\n\"+Corpus_as_one_string)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "open(os.path.join(directory, Corpus_as_one_string+\".txt\",\"w+\")\n",
    "    write(Corpus_as_one_string+\"\\n\"+Corpus_as_one_string)\n",
    "    close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (<ipython-input-59-af5980acda18>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-59-af5980acda18>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    f=directory\\Corpus_as_one_string.\u001b[0m\n\u001b[0m                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "f=directory\\Corpus_as_one_string.\n",
    "f.open(f,\"w+\") \n",
    "f.write (Corpus_as_one_string)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "Keywords_for_whole_corpus=keywords(Corpus_as_one_string, words=3, scores=True, lemmatize=True, deacc=True)\n",
    "# Other options are split=False, pos_filter=('NN', 'JJ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('included', 0.2100281475342915), ('safety', 0.16399909537537963), ('designated', 0.15641899206309517)]\n"
     ]
    }
   ],
   "source": [
    "print(Keywords_for_whole_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "Graph_for_whole_corpus=get_graph(Corpus_as_one_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<gensim.summarization.graph.Graph object at 0x10c4088e0>\n"
     ]
    }
   ],
   "source": [
    "print(Graph_for_whole_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keywords for whole library\n",
    "\n",
    "sorted(g.nodes())\n",
    "\n",
    "igraph plot\n",
    "\n",
    "igraph scores as 2d plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Keywords_for_whole_corpus =keywords(Corpus_as_one_string, word_count=5)\n",
    "print(Keywords_for_whole_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = PorterStemmer()\n",
    "p.stem_documents([Keywords_for_whole_corpus])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IDENTIFY TOKENS AND MAKE-UP DICTIONARY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove common words and tokenize.# Here we can add in some odd words we find in the output, or use the NLTK list\n",
    "stoplist = set('for a of the and to in \\uf06e  • \\uf0b7 \\uf0b7 \\uf06e uf09 \\uf09f'.split())\n",
    "Tokens_in_Corpus = [[word for word in summary.lower().split() if word not in stoplist]\n",
    "         for summary in Corpus_of_Summaries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove words that appear only once\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for text in Tokens_in_Corpus:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "Frequent_Tokens_in_Corpus= [[token for token in text if frequency[token] > 1] for text in Tokens_in_Corpus]\n",
    "\n",
    "from pprint import pprint  # pretty-printer\n",
    "pprint(Frequent_Tokens_in_Corpus[4:5]) #these slices of lists go up to before the higher number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dictionary, then map from ids to dictionary\n",
    "dictionary = corpora.Dictionary(Frequent_Tokens_in_Corpus)\n",
    "print(dictionary,\"\\n\\n\")\n",
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CREATE BAG OF WORDS MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ie. a list of a list. For each document, we have a list of word frequency for each dictionary item\n",
    "BAG_OF_WORDS_MODEL = [dictionary.doc2bow(text) for text in Frequent_Tokens_in_Corpus]\n",
    "for c in BAG_OF_WORDS_MODEL:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Quick start tutorial. \n",
    "\"Now that we have vectorized our corpus we can begin to transform it using models. We use model as an abstract term referring to a transformation from one document representation to another. In gensim documents are represented as vectors so a model can be thought of as a transformation between two vector spaces. The details of this transformation are learned from the training corpus.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CREATE TF-IDF MODEL**\n",
    "One simple example of a model is tf-idf. The tf-idf model transforms vectors from the bag-of-words representation to a vector space where the frequency counts are weighted according to the relative rarity of each word in the corpus.\n",
    "Let's initialize the tf-idf model, training it on our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "TFIDF_MODEL= models.TfidfModel(BAG_OF_WORDS_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CREATE TOPIC MODEL via LSI** via CBOW AND TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSI APPLIED ON TOP ON TFIDF\n",
    "#now applying an LSI to the first corpus, by working on top of its representation as a TFIDF\n",
    "# here we have created a two dim LSI space, like Deerwesters 1990 example\n",
    "#Presumably we could create one on top of just the CBOW too\n",
    "lsi_from_TFIDF= models.LsiModel(TFIDF_APPLIED_TO_TRAINING_CORPUS, id2word=dictionary, num_topics=3) # initialize an LSI transformation\n",
    "\n",
    "#It is correct how it has this odd double-barrelled structure: \n",
    "#model = LsiModel(common_corpus, id2word=common_dictionary)\n",
    "# >>> vectorized_corpus = model[common_corpus]  # vectorize input copus in BoW format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspect the topics\n",
    "lsi_from_TFIDF.print_topics(num_topics=-1, num_words=20) #-1 means show all topics .In significance order. Remember also _ve Contribs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get a single topic as a formatted string with print_topic(topicno, topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to get as array use lsi.get_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can Update model with new corpus using add_documents(corpus, chunksize=None, decay=None)\n",
    "#can also save the LSI model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FINDING VECTOR REPRESENTATION OF A WHOLE OLD OR NEW CORPUS**\n",
    "To prepare for similarity queries, we need to enter all documents which we want to compare against subsequent queries. In our case, they are the same documents used for training LSI, converted to 3-D LSA space. But that’s only incidental, we might also be indexing a different corpus altogether."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   **REPRESENTATION OF OLD CORPUS: TFIDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#now moved onto Topic and Transformations tutorial\n",
    "#apply tfidf to the trained corpus\n",
    "TFIDF_APPLIED_TO_TRAINING_CORPUS = TFIDF_MODEL[BAG_OF_WORDS_MODEL]\n",
    "for doc in TFIDF_APPLIED_TO_TRAINING_CORPUS:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   **REPRESENTATION OF OLD CORPUS: LSI FROM TFIDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a double wrapper over the original corpus: bow->tfidf->fold-in-lsi\n",
    "TOPIC_MODEL_LSI_from_TFIDF_APPLIED_TO_CORPUS = lsi_from_TFIDF[TFIDF_APPLIED_TO_TRAINING_CORPUS] \n",
    "\n",
    "#particular documents aligned to particular topics\n",
    "for doc in TOPIC_MODEL_LSI_from_TFIDF_APPLIED_TO_CORPUS: # both bow->tfidf and tfidf->lsi transformations are actually executed here, on the fly\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPTIONAL\n",
    "#this model can now be applied to another corpus other than the training one, not just individaul documents\n",
    "#i have not pulled in a second corpus but this is how you would do it. Note you pull in a corpus (processed as above), not just docs. \n",
    "#corpus2nd_tfidf = TFIDF_MODEL[corpus2nd]\n",
    "# for doc in corpus2nd_tfidf:\n",
    "#   print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FINDING VECTOR REPRESENTATION OF A SINGLE NEW DOCUMENT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Up above, we had a CBOW representation of each document\n",
    "#We can convert documents to that vector space,once tokenized\n",
    "\n",
    "# eg.This is the announcement of the Sellafield partner programme. https://www.gov.uk/government/news/sellafield-ltd-awards-20-year-project-partnership\n",
    "#Which ONR document is most relevant to this contract ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get(\"https://www.gov.uk/government/news/sellafield-ltd-awards-20-year-project-partnership\")\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "new_doc=strip_multiple_whitespaces(soup.get_text())\n",
    "print(new_doc)\n",
    "\n",
    "#page = requests.get(\"https://www.gov.uk/government/news/nda-sets-out-its-grand-challenges\")\n",
    "#page.content[1:300]\n",
    "# need to find which tag works well with this approach. p does not work well with this NEC text \n",
    "#soup.find_all('p')[5].get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_doc=new_doc[0:5487]\n",
    "print(new_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we will be considering cosine similarity to determine the similarity of two vectors. Cosine similarity is a standard measure in Vector Space Modeling, but wherever the vectors represent probability distributions, different similarity measures may be more appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   **REPRESENTATION OF NEW DOCUMENT: CBOW ONLY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert tokenized documents to vector\n",
    "new_vec_CBOW = dictionary.doc2bow(new_doc.lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_vec_CBOW)  # only those words that match up are given a dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   **REPRESENTATION OF NEW DOCUMENT: TFIDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the query to LSI space (based on TFIDF) \n",
    "new_vec_TFIDF=TFIDF_MODEL[new_vec_CBOW]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   **REPRESENTATION OF NEW DOCUMENT: LSI via TFIDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vec_lsi_fromTFIDF = lsi_from_TFIDF[new_vec_TFIDF]\n",
    "print(new_vec_lsi_fromTFIDF)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**COSINE SIMILARITY**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "moved onto Similarity search tutorial.\n",
    "Based on this new doc query,we would like to sort our corpus documents in decreasing order of relevance to this query. Unlike modern search engines, here we only concentrate on a single aspect of possible similarities—on apparent semantic relatedness of their texts (words). No hyperlinks, no random-walk static ranks, just a semantic extension overthe boolean keyword match:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   **LSI VIA TFIDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSI APPLIED ON TOP ON TFIDF\n",
    "index = similarities.MatrixSimilarity(lsi_from_TFIDF[TFIDF_APPLIED_TO_TRAINING_CORPUS]) # transform corpus to LSI space and index it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = index[new_vec_lsi_fromTFIDF] # perform a similarity query against the corpus BASED ON LSI - TDFIDF\n",
    "print(list(enumerate(sims))) # print (document_number, document_similarity) 2-tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine measure returns similarities in the range <-1, 1> (the greater, the more similar), so that the first document has a score of 0.99809301 etc.\n",
    "\n",
    "With some standard Python magic we sort these similarities into descending order, and obtain the final answer to the query for Sellafield PPP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "print(sims) # print sorted (document number, similarity score) 2-tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Most like\n",
    "print (Corpus_of_Summaries[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (Corpus_of_Summaries[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (Corpus_of_Summaries[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Least like\n",
    "print (Corpus_of_Summaries[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (Corpus_of_Summaries[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   **TFIDF ONLY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now the same but with TFIDF model\n",
    "index = similarities.MatrixSimilarity(TFIDF_APPLIED_TO_TRAINING_CORPUS)\n",
    "new_vec_TFIDF = TFIDF_MODEL[new_vec_CBOW] # convert the query to LSI space (based on TFIDF)\n",
    "print(new_vec_TFIDF)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = index[new_vec_TFIDF] \n",
    "print(list(enumerate(sims)))\n",
    "sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "print (\"\\n\")\n",
    "print (sims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Using keywords into Neo4j concurrence...**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acknowledgements\n",
    "\n",
    "This project relies extensively on the Gensim library, and the [examples](https://radimrehurek.com/gensim/auto_examples/index.html) provided by its creator Radim Hurek. I have done nothing more than apply a little of this to Portfolio management. The examples cited above would be the best way to get a full introduction to the capabilities of Gensim\n",
    "\n",
    "\n",
    "@inproceedings{rehurek_lrec,\n",
    "      title = {{Software Framework for Topic Modelling with Large Corpora}},\n",
    "      author = {Radim {\\v R}eh{\\r u}{\\v r}ek and Petr Sojka},\n",
    "      booktitle = {{Proceedings of the LREC 2010 Workshop on New\n",
    "           Challenges for NLP Frameworks}},\n",
    "      pages = {45--50},\n",
    "      year = 2010,\n",
    "      month = May,\n",
    "      day = 22,\n",
    "      publisher = {ELRA},\n",
    "      address = {Valletta, Malta},\n",
    "      note={\\url{http://is.muni.cz/publication/884893/en}},\n",
    "      language={English}\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
