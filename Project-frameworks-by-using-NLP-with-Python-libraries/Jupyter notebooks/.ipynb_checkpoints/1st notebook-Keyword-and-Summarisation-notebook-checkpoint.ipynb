{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<img src=\"xx.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1st Notebook example: Keyword and Topic modelling\n",
    "\n",
    "**[Purpose](#Purpose)** | **[Motivation](#Motivation)** |**[Features](#Features)** |**[Quick start](#Quick-start)** |\n",
    "\n",
    "# Purpose \n",
    "\n",
    "**For any set of portfolio documents, automatically generate keywords and summaries per document, and generate a set of the main topics found. Accordingly, generate a data-model for the portfolio** \n",
    "\n",
    "# Motivation\n",
    "\n",
    "This is the first notebook to get started on. It provides basic code that can be applied directly to  any portfolio documents. Here this code is worked through with an example of understanding the implications of the regularatory environment on project delivery in the nuclear sector.This can be replaced as the user generates her own results. \n",
    "\n",
    "For simplicity, this one starts from documents as text files and internet pages. The second example shows how to generate text files from pdfs. \n",
    "\n",
    "Additional features can be sought in the other notebooks once results have been generated for your portfolio documents. \n",
    "\n",
    "# Features\n",
    "- Summarises each document\n",
    "- finds keywords for each document\n",
    "- creates a similarity-search: for any new paragraph, it finds the most similar documents from the library\n",
    "- (propose a data-model for the portfolio, based upon these findings) ***(Status: incomplete)***\n",
    "\n",
    "# Quick-start\n",
    "\n",
    "It is suggested to move straight down to (#How-to-use) to get started.\n",
    "\n",
    "(Attributes and adjustments are included immediately below for ease of reference, but best consulted after using the notebook).\n",
    "\n",
    "The user can start from the markdown file (xx) showing and explaining the code, or can work through the notebook directly (xx)\n",
    "\n",
    "## Attributes\n",
    "\n",
    "- Uses GENSIM library\n",
    "- Starts from Text file rather than PDF\n",
    "- also starts from Internet page\n",
    "- creates corpus at Document level rather than paragraph level\n",
    "- for Topic model, starts from Bag of Words model, as an interim step only\n",
    "- It uses TFIDF model only as an interim step to LSI only\n",
    "- for similarity-search, it uses cosine similarity, based upon LSI model\n",
    "\n",
    "# Attributes you can find in the other notebooks\n",
    "- Using other libraries than Gensim\n",
    "- Starting from Text file\n",
    "- creates corpus at paragraph level\n",
    "- uses Bag of Words model to generate results directly for similarity search\n",
    "\n",
    "# Easy adjustments that can be made, but have not been shown here or elsewhere\n",
    "- for Topic model, uses only Bag of Words model \n",
    "- for similarity-search, using cosine similarity, based upon TFIDF model\n",
    "- for topic model, uses methods other than LSI. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How-to-use\n",
    "\n",
    "## Installation\n",
    "\n",
    "First Check installation has been made, as per the [READme](https://github.com/lawrencerowland/Data-Model-for-Project-Frameworks/blob/master/Project-frameworks-by-using-NLP-with-Python-libraries/README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.summarization import summarize\n",
    "from gensim.summarization import keywords\n",
    "from gensim.parsing.preprocessing import strip_multiple_whitespaces\n",
    "from gensim import corpora, models, similarities\n",
    "# Note that Smart_open.gcs may not import successfully depending on Gensim set up. This doesnt matter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Directory to find the portfolio text files\n",
    "\n",
    "This outputs the names of the 16 text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Commissioning of security systems and infrastructure cns-tast-gd-4.4 cns-tast-gd-4.4.pdf.txt', 'Construction Assurance ns-tast-gd-076.pdf.txt', 'Control of processes involving nuclear matter ns-tast-gd-023.pdf.txt', 'Decommissioning ns-tast-gd-026.txt', 'Design Safety Assurance ns-tast-gd-057.pdf.txt', 'Fundamental Principles ns-tast-gd-004.pdf.txt', 'Guidance on the Demonstration of ALARP ns-tast-gd-005.pdf.txt', 'Management of Radioactive material ns-tast-gd-024.pdf.txt', 'nuclear construction sites cns-tast-gd-6.6.pdf.txt', 'Organisational Change ns-tast-gd-048.pdf.txt', 'oversight of items or services cns-tast-gd-4.3.pdf.txt', 'PM_guidance_for_Energy_Projects.txt', 'Probabilistic Safety Analysis.pdf.txt', 'Procedure Design and Administrative Controls.pdf.txt', 'Procurement cns-tast-gd-4.1 cns-tast-gd-4.1.pdf.txt', 'Reliability and resilience of the security system cns-tast-gd-5.1 cns-tast-gd-5.1.pdf.txt', 'Supplier capability cns-tast-gd-4.2.pdf.txt']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "directory = '/Users/lawrence/Documents/GitHub/Data-Model-for-Project-Frameworks/Project-frameworks-by-using-NLP-with-Python-libraries/Text-files-as-generated-by-PDF-Miner'\n",
    "print (os.listdir(directory))\n",
    "# Change directory location for your particular set-up. Or if you want to just re-run this nuclear example, then you just need to change the reference to the high-level directories\n",
    "# If a mac, can copy the directory location by opening the Inspector for the folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect any Internet pages \n",
    "\n",
    "This is an option. \n",
    "\n",
    "In order to compare the results with the Orange results, I have not run this block of code. \n",
    "\n",
    "In addition to the text files you have collected, there may be one or two internet pages which you want to add to the analysis. \n",
    "I have added this partly to show what happens when a document is added which is about the same subject, but comes at the subject from a very different angle. \n",
    "\n",
    "If you wish to collect many Internet pages, you will need to construct a loop.  You also will need to check how well the text is extracted from  particular sources, and consult the Python Requests module documentation if it needs adjustment. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "text = requests.get('https://archive.org/stream/ProjectManagementForTheOilAndGasIndustry/ProjectManagementForTheOilAndGasIndustry_djvu.txt').text\n",
    "text=text[70000:300000] #here I have stripped the front text by inspection only.\n",
    "#text=strip_multiple_whitespaces(text)  #may wish to turn this off for readability\n",
    "filename=\"PM_guidance_for_Energy_Projects\"\n",
    "f= open(filename+\".txt\",\"w+\") \n",
    "f.write(filename+\"\\n\"+text)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GENSIM summary, keywords\n",
    "Corpus_of_Summaries =[]\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.txt'):\n",
    "        with open(os.path.join(directory, filename)) as f:\n",
    "            \n",
    "            content = f.read()\n",
    "            \n",
    "            summary=(filename+'Summary\\n'+summarize(content, ratio=0.01)) #word_count=20\n",
    "            key_words=keywords(content, ratio=0.007)\n",
    "            \n",
    "            print ('\\n\\nFile',filename)\n",
    "            print ('\\nSummary:',summary,\"\\n\")\n",
    "            print ('\\nKeywords:',key_words)\n",
    "            \n",
    "            Corpus_of_Summaries.append(summary)\n",
    "            \n",
    "            #print (content[0:100]) # testing the content is coming through\n",
    "            # print(repr(summary)) #alternate version showing line breaks etc\n",
    "    \n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Corpus_of_Summaries[2]) # Could equally do this w full content, but starting simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IDENTIFY TOKENS AND MAKE-UP DICTIONARY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove common words and tokenize.# Here we can add in some odd words we find in the output, or use the NLTK list\n",
    "stoplist = set('for a of the and to in \\uf06e  â€¢ \\uf0b7 \\uf0b7 \\uf06e uf09 \\uf09f'.split())\n",
    "Tokens_in_Corpus = [[word for word in summary.lower().split() if word not in stoplist]\n",
    "         for summary in Corpus_of_Summaries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove words that appear only once\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for text in Tokens_in_Corpus:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "Frequent_Tokens_in_Corpus= [[token for token in text if frequency[token] > 1] for text in Tokens_in_Corpus]\n",
    "\n",
    "from pprint import pprint  # pretty-printer\n",
    "pprint(Frequent_Tokens_in_Corpus[4:5]) #these slices of lists go up to before the higher number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dictionary, then map from ids to dictionary\n",
    "dictionary = corpora.Dictionary(Frequent_Tokens_in_Corpus)\n",
    "print(dictionary,\"\\n\\n\")\n",
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CREATE BAG OF WORDS MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ie. a list of a list. For each document, we have a list of word frequency for each dictionary item\n",
    "BAG_OF_WORDS_MODEL = [dictionary.doc2bow(text) for text in Frequent_Tokens_in_Corpus]\n",
    "for c in BAG_OF_WORDS_MODEL:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Quick start tutorial. \n",
    "\"Now that we have vectorized our corpus we can begin to transform it using models. We use model as an abstract term referring to a transformation from one document representation to another. In gensim documents are represented as vectors so a model can be thought of as a transformation between two vector spaces. The details of this transformation are learned from the training corpus.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CREATE TF-IDF MODEL**\n",
    "One simple example of a model is tf-idf. The tf-idf model transforms vectors from the bag-of-words representation to a vector space where the frequency counts are weighted according to the relative rarity of each word in the corpus.\n",
    "Let's initialize the tf-idf model, training it on our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "TFIDF_MODEL= models.TfidfModel(BAG_OF_WORDS_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CREATE TOPIC MODEL via LSI** via CBOW AND TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSI APPLIED ON TOP ON TFIDF\n",
    "#now applying an LSI to the first corpus, by working on top of its representation as a TFIDF\n",
    "# here we have created a two dim LSI space, like Deerwesters 1990 example\n",
    "#Presumably we could create one on top of just the CBOW too\n",
    "lsi_from_TFIDF= models.LsiModel(TFIDF_APPLIED_TO_TRAINING_CORPUS, id2word=dictionary, num_topics=3) # initialize an LSI transformation\n",
    "\n",
    "#It is correct how it has this odd double-barrelled structure: \n",
    "#model = LsiModel(common_corpus, id2word=common_dictionary)\n",
    "# >>> vectorized_corpus = model[common_corpus]  # vectorize input copus in BoW format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspect the topics\n",
    "lsi_from_TFIDF.print_topics(num_topics=-1, num_words=20) #-1 means show all topics .In significance order. Remember also _ve Contribs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get a single topic as a formatted string with print_topic(topicno, topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to get as array use lsi.get_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can Update model with new corpus using add_documents(corpus, chunksize=None, decay=None)\n",
    "#can also save the LSI model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FINDING VECTOR REPRESENTATION OF A WHOLE OLD OR NEW CORPUS**\n",
    "To prepare for similarity queries, we need to enter all documents which we want to compare against subsequent queries. In our case, they are the same documents used for training LSI, converted to 3-D LSA space. But thatâ€™s only incidental, we might also be indexing a different corpus altogether."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   **REPRESENTATION OF OLD CORPUS: TFIDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#now moved onto Topic and Transformations tutorial\n",
    "#apply tfidf to the trained corpus\n",
    "TFIDF_APPLIED_TO_TRAINING_CORPUS = TFIDF_MODEL[BAG_OF_WORDS_MODEL]\n",
    "for doc in TFIDF_APPLIED_TO_TRAINING_CORPUS:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   **REPRESENTATION OF OLD CORPUS: LSI FROM TFIDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a double wrapper over the original corpus: bow->tfidf->fold-in-lsi\n",
    "TOPIC_MODEL_LSI_from_TFIDF_APPLIED_TO_CORPUS = lsi_from_TFIDF[TFIDF_APPLIED_TO_TRAINING_CORPUS] \n",
    "\n",
    "#particular documents aligned to particular topics\n",
    "for doc in TOPIC_MODEL_LSI_from_TFIDF_APPLIED_TO_CORPUS: # both bow->tfidf and tfidf->lsi transformations are actually executed here, on the fly\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPTIONAL\n",
    "#this model can now be applied to another corpus other than the training one, not just individaul documents\n",
    "#i have not pulled in a second corpus but this is how you would do it. Note you pull in a corpus (processed as above), not just docs. \n",
    "#corpus2nd_tfidf = TFIDF_MODEL[corpus2nd]\n",
    "# for doc in corpus2nd_tfidf:\n",
    "#   print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FINDING VECTOR REPRESENTATION OF A SINGLE NEW DOCUMENT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Up above, we had a CBOW representation of each document\n",
    "#We can convert documents to that vector space,once tokenized\n",
    "\n",
    "# eg.This is the announcement of the Sellafield partner programme. https://www.gov.uk/government/news/sellafield-ltd-awards-20-year-project-partnership\n",
    "#Which ONR document is most relevant to this contract ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get(\"https://www.gov.uk/government/news/sellafield-ltd-awards-20-year-project-partnership\")\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "new_doc=strip_multiple_whitespaces(soup.get_text())\n",
    "print(new_doc)\n",
    "\n",
    "#page = requests.get(\"https://www.gov.uk/government/news/nda-sets-out-its-grand-challenges\")\n",
    "#page.content[1:300]\n",
    "# need to find which tag works well with this approach. p does not work well with this NEC text \n",
    "#soup.find_all('p')[5].get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_doc=new_doc[0:5487]\n",
    "print(new_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we will be considering cosine similarity to determine the similarity of two vectors. Cosine similarity is a standard measure in Vector Space Modeling, but wherever the vectors represent probability distributions, different similarity measures may be more appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   **REPRESENTATION OF NEW DOCUMENT: CBOW ONLY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert tokenized documents to vector\n",
    "new_vec_CBOW = dictionary.doc2bow(new_doc.lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_vec_CBOW)  # only those words that match up are given a dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   **REPRESENTATION OF NEW DOCUMENT: TFIDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the query to LSI space (based on TFIDF) \n",
    "new_vec_TFIDF=TFIDF_MODEL[new_vec_CBOW]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   **REPRESENTATION OF NEW DOCUMENT: LSI via TFIDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vec_lsi_fromTFIDF = lsi_from_TFIDF[new_vec_TFIDF]\n",
    "print(new_vec_lsi_fromTFIDF)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**COSINE SIMILARITY**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "moved onto Similarity search tutorial.\n",
    "Based on this new doc query,we would like to sort our corpus documents in decreasing order of relevance to this query. Unlike modern search engines, here we only concentrate on a single aspect of possible similaritiesâ€”on apparent semantic relatedness of their texts (words). No hyperlinks, no random-walk static ranks, just a semantic extension overthe boolean keyword match:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   **LSI VIA TFIDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSI APPLIED ON TOP ON TFIDF\n",
    "index = similarities.MatrixSimilarity(lsi_from_TFIDF[TFIDF_APPLIED_TO_TRAINING_CORPUS]) # transform corpus to LSI space and index it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = index[new_vec_lsi_fromTFIDF] # perform a similarity query against the corpus BASED ON LSI - TDFIDF\n",
    "print(list(enumerate(sims))) # print (document_number, document_similarity) 2-tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine measure returns similarities in the range <-1, 1> (the greater, the more similar), so that the first document has a score of 0.99809301 etc.\n",
    "\n",
    "With some standard Python magic we sort these similarities into descending order, and obtain the final answer to the query for Sellafield PPP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "print(sims) # print sorted (document number, similarity score) 2-tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Most like\n",
    "print (Corpus_of_Summaries[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (Corpus_of_Summaries[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (Corpus_of_Summaries[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Least like\n",
    "print (Corpus_of_Summaries[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (Corpus_of_Summaries[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   **TFIDF ONLY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now the same but with TFIDF model\n",
    "index = similarities.MatrixSimilarity(TFIDF_APPLIED_TO_TRAINING_CORPUS)\n",
    "new_vec_TFIDF = TFIDF_MODEL[new_vec_CBOW] # convert the query to LSI space (based on TFIDF)\n",
    "print(new_vec_TFIDF)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = index[new_vec_TFIDF] \n",
    "print(list(enumerate(sims)))\n",
    "sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "print (\"\\n\")\n",
    "print (sims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Using keywords into Neo4j concurrence...**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acknowledgements\n",
    "\n",
    "This project relies extensively on the Gensim library, and the [examples](https://radimrehurek.com/gensim/auto_examples/index.html) provided by its creator Radim Hurek. I have done nothing more than apply a little of this to Portfolio management. The examples cited above would be the best way to get a full introduction to the capabilities of Gensim\n",
    "\n",
    "\n",
    "@inproceedings{rehurek_lrec,\n",
    "      title = {{Software Framework for Topic Modelling with Large Corpora}},\n",
    "      author = {Radim {\\v R}eh{\\r u}{\\v r}ek and Petr Sojka},\n",
    "      booktitle = {{Proceedings of the LREC 2010 Workshop on New\n",
    "           Challenges for NLP Frameworks}},\n",
    "      pages = {45--50},\n",
    "      year = 2010,\n",
    "      month = May,\n",
    "      day = 22,\n",
    "      publisher = {ELRA},\n",
    "      address = {Valletta, Malta},\n",
    "      note={\\url{http://is.muni.cz/publication/884893/en}},\n",
    "      language={English}\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
